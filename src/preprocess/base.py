#!/usr/bin/env python
# encoding: utf-8
'''
preprocess.base -- shortdesc

preprocess.base is a description

It defines classes_and_methods

@author:     user_name
            
@copyright:  2013 organization_name. All rights reserved.
            
@license:    license

@contact:    user_email
@deffield    updated: Updated
'''

import sys
import os
import glob
import nipype.pipeline.engine
import nipype.interfaces.utility
import nipype.interfaces.io

from nipype.interfaces.freesurfer.preprocess import ReconAll

from optparse import OptionParser

__all__ = []
__version__ = 0.1
__date__ = '2013-09-30'
__updated__ = '2013-09-30'

DEBUG = 1
TESTRUN = 0
PROFILE = 0
OUTPUT_DIR = '/work/01551/ccumba/output'

#@author Satrajit Ghosh
def get_subjectinfo(subject_id, base_dir, task_id, model_id):
    """Get info for a given subject

    Parameters
    ----------
    subject_id : string
        Subject identifier (e.g., sub001)
    base_dir : string
        Path to base directory of the dataset
    task_id : int
        Which task to process
    model_id : int
        Which model to process

    Returns
    -------
    run_ids : list of ints
        Run numbers
    conds : list of str
        Condition names
    TR : float
        Repetition time
    """
    from glob import glob
    import os
    import numpy as np
    condition_info = []
    cond_file = os.path.join(base_dir, 'models', 'model%03d' % model_id,
                             'condition_key.txt')
    with open(cond_file, 'rt') as fp:
        for line in fp:
            info = line.strip().split()
            condition_info.append([info[0], info[1], ' '.join(info[2:])])
    if len(condition_info) == 0:
        raise ValueError('No condition info found in %s' % cond_file)
    taskinfo = np.array(condition_info)
    n_tasks = len(np.unique(taskinfo[:, 0]))
    conds = []
    run_ids = []
    if task_id > n_tasks:
        raise ValueError('Task id %d does not exist' % task_id)
    for idx in range(n_tasks):
        taskidx = np.where(taskinfo[:, 0] == 'task%03d' % (idx + 1))
        conds.append([condition.replace(' ', '_') for condition
                      in taskinfo[taskidx[0], 2]])
        files = glob(os.path.join(base_dir,
                                  subject_id,
                                  'BOLD',
                                  'task%03d_run*' % (idx + 1)))
        run_ids.insert(idx, range(1, len(files) + 1))
    TR = np.genfromtxt(os.path.join(base_dir, 'scan_key.txt'))[1]
    return run_ids[task_id - 1], conds[task_id - 1], TR


def main(argv=None):
    '''Command line options.'''
    
    program_name = os.path.basename(sys.argv[0])
    program_version = "v0.1"
    program_build_date = "%s" % __updated__
 
    program_version_string = '%%prog %s (%s)' % (program_version, program_build_date)
    #program_usage = '''usage: spam two eggs''' # optional - will be autogenerated by optparse
    program_longdesc = '''''' # optional - give further explanation about what the program does
    program_license = "Copyright 2013 user_name (organization_name)                                            \
                Licensed under the Apache License 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0"
 
    if argv is None:
        argv = sys.argv[1:]
    try:
        # setup option parser
        parser = OptionParser(version=program_version_string, epilog=program_longdesc, description=program_license)
        parser.add_option("-i", "--in", dest="infile", help="set input path [default: %default]", metavar="FILE")
        parser.add_option("-o", "--out", dest="outfile", help="set output path [default: %default]", metavar="FILE")
        parser.add_option("-v", "--verbose", dest="verbose", action="count", help="set verbosity level [default: %default]")
        parser.add_option("-d", "--datadir", dest="data_directory", help="set project data directory [default: %default/ ]",
                          metavar="DIRECTORY")
        parser.add_option("-s", "--subject", dest="subject", help="set subject directory [default: %default]", 
                          metavar="DIRECTORY")
        parser.add_option("-m", "--model", dest="model_id", help="set the model to perform analysis on [default: %default]",
                          metavar="NUMBER")
        parser.add_option("-t", "--task", dest="task_id", help="set the task to perform analysis on [default: %default]",
                          metavar="NUMBER")
        parser.add_option("-w", "--work", dest="work_directory", help="set the directory to store transitional files [default: %default]",
                          metavar="DIRECTORY")
        parser.add_option("-g", "--getdata", dest="get_data", help="get data from XNAT [default: %default]", action="store_true")
         
        # set defaults
        parser.set_defaults(outfile="./out.txt", infile="./in.txt", work_directory="./work", task_id=1,
                            model_id=1, subject="all", get_data=False, data_directory=".")
        # process options
        (opts, args) = parser.parse_args(argv)
        print "wat"
        if opts.verbose > 0:
            print("verbosity level = %d" % opts.verbose)
        if opts.infile:
            print("infile = %s" % opts.infile)
        if opts.outfile:
            print("outfile = %s" % opts.outfile)
        if opts.subject == "all":
            opts.subject = None
        
        # MAIN BODY #
    except Exception, e:
        indent = len(program_name) * " "
        sys.stderr.write(program_name + ": " + repr(e) + "\n")
        sys.stderr.write(indent + "  for help use --help\n")
        return 2
    
    analyze_openfmri_dataset(opts.data_directory, subject=opts.subject, model_id=opts.model_id, task_id=opts.task_id,
                             work_directory=opts.work_directory, xnat_datasource=opts.get_data)


def analyze_openfmri_dataset(data_directory, subject=None, model_id=None, task_id=None, work_directory=None, xnat_datasource=False):
    """
    @TODO - docs
    
    @param data_directory - directory containing subject folders
    @param subject - directory within data_directory containing a single subjects data
    @param model_id - the model number to run
    @param task_id - the task to run
    @param work_directory - directory to store transitional files
    """
    subjects = [path.split(os.path.sep)[-1] for path in glob.glob(os.path.join(data_directory, 'sub*'))]
    
    infosource = nipype.pipeline.engine.Node(
                    nipype.interfaces.utility.IdentityInterface(fields=["subject_id",
                                                                        "model_id",
                                                                        "task_id"]),
                                                                name="infosource")
    
    if subject is None:
        infosource.iterables = [("subject_id", subjects),
                                ("model_id", [model_id])]
    else:
        infosource.iterables = [("subject_id", [subjects[subjects.index(subject)]]),
                                ("model_id", [model_id]),
                                ("task_id", [task_id])
                                ]
    
    subject_info = nipype.pipeline.engine.Node(
                        nipype.interfaces.utility.Function(input_names=["subject_id", "base_dir",
                                                                        "task_id", "model_id"],
                                                           output_names=["run_id", "conds", "TR"],
                                                           function=get_subjectinfo),
                                               name="subjectinfo")
    
    subject_info.inputs.base_dir = data_directory

    if xnat_datasource:
        datasource = nipype.pipeline.engine.Node(
                        nipype.interfaces.io.XNATSource(infields=["project", "subject_id"],
                                                        outfields=["anat","bold","behav"]),
                                                  name="datasource")
        datasource.inputs.template = "*dcm"
        datasource.inputs.query_template = "/xnat-irc/data/archive/projects/%s/subjects/*/experiments/*/scans/ALL/files"
        
    
    datasource = nipype.pipeline.engine.Node(
                    nipype.interfaces.io.DataGrabber(infields=["subject_id", "run_id",
                                                               "task_id", "model_id"],
                                                     outfields=["anat", "bold", "behav"]),
                                             name="datasource")
    
    datasource.inputs.base_directory = data_directory
    datasource.inputs.template = "*"
    datasource.inputs.field_template = {"anat" : "%s/anatomy/highres001.nii.gz",
                                        "bold" : "%s/BOLD/task%03d_r*/bold.nii.gz",
                                        "behav" : "%s/model/model%03d/onsets/task%03d_run%03d/cond*.txt"
                                        }
    datasource.inputs.template_args = {"anat" : [["subject_id"]],
                                       "bold" : [["subject_id", "task_id"]],
                                       "behav" : [["subject_id", "model_id", "task_id", "run_id"]]
                                       }
    datasource.inputs.sort_filelist = True
    
    workflow = nipype.pipeline.engine.Workflow(name="openfmri")
    
    if work_directory is None:
        work_directory = os.path.join(os.getcwd(),'working')
        
    workflow.base_dir = work_directory
    
    workflow.connect(infosource, "subject_id", subject_info, "subject_id")
    workflow.connect(infosource, "model_id", subject_info, "model_id")
    workflow.connect(infosource, "task_id", subject_info, "task_id")
    workflow.connect(infosource, "subject_id", datasource, "subject_id")
    workflow.connect(infosource, "model_id", datasource, "model_id")
    workflow.connect(infosource, "task_id", datasource, "task_id")    
    workflow.connect(subject_info, 'run_id', datasource, 'run_id')
    
    cortical_reconstruction = nipype.pipeline.engine.Node(
                                interface=ReconAll(),
                                name="cortical_reconstruction")
    
    workflow.connect(datasource, "anat", cortical_reconstruction, "T1_files" )
    
    datasink = nipype.pipeline.engine.Node(nipype.interfaces.io.DataSink(), name="datasink")
    datasink.inputs.base_directory = OUTPUT_DIR
    
    workflow.connect(cortical_reconstruction, 'subject_id', datasink, 'container')
    workflow.connect(cortical_reconstruction, 'T1', datasink, 'highres')
    
    
    workflow.run(plugin="SGE", plugin_args={"qsub_args":("-l h_rt=1:00:00 -q normal -A Analysis_Lonestar " 
                                            "-pe 12way 24 -M chad.cumba@mail.utexas.edu")})

if __name__ == "__main__":
    if DEBUG:
        pass
    if TESTRUN:
        import doctest
        doctest.testmod()
    if PROFILE:
        import cProfile
        import pstats
        profile_filename = 'preprocess.base_profile.txt'
        cProfile.run('main()', profile_filename)
        statsfile = open("profile_stats.txt", "wb")
        p = pstats.Stats(profile_filename, stream=statsfile)
        stats = p.strip_dirs().sort_stats('cumulative')
        stats.print_stats()
        statsfile.close()
        sys.exit(0)
    sys.exit(main())
